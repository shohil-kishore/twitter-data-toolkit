{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook to import 4 files generated from Twitter Data Toolkit, extract nested data into new rows, and merge all data into one CSV/Excel file. Requires Python 3.x, Numpy and Pandas. Messy but it works, I'll create a function for this later on. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 4 files created by Twitter Data Toolkit\n",
    "new_users = pd.read_json(r\"users.json\")\n",
    "ext_tweets = pd.read_json(r\"ext-tweets.json\")\n",
    "places = pd.read_json(r\"places.json\")\n",
    "tweets = pd.read_json(r\"tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users: Merge and extract nested data, drop irrelevant rows, rename new rows. \n",
    "for index, row in new_users.iterrows():\n",
    "    network = row['public_metrics']\n",
    "    \n",
    "    # Extract followers.\n",
    "    new_users.at[index,'followers'] = network['followers_count']\n",
    "\n",
    "    # Extract friends.\n",
    "    new_users.at[index,'following'] = network['following_count']\n",
    "    \n",
    "    # Extract total tweets.\n",
    "    new_users.at[index,'total_tweets'] = network['tweet_count']\n",
    "    \n",
    "new_users = new_users.drop(['public_metrics','entities','pinned_tweet_id','profile_image_url','withheld'], axis=1)\n",
    "\n",
    "new_users = new_users.rename(columns={\"created_at\": \"profile_created\", \"description\": \"profile_desc\", \"location\":\"profile_loc\", \"id\": \"author_id\"})\n",
    "\n",
    "new_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended Tweets: Merge and extract nested data, drop irrelevant rows, rename new rows.\n",
    "for index, row in ext_tweets.iterrows():\n",
    "    network = row['public_metrics']\n",
    "    \n",
    "    # Extract RTs.\n",
    "    ext_tweets.at[index,'retweets'] = network['retweet_count']\n",
    "    \n",
    "    # Extract replies.\n",
    "    ext_tweets.at[index,'replies'] = network['reply_count']\n",
    "    \n",
    "    # Extract replies.\n",
    "    ext_tweets.at[index,'likes'] = network['like_count']\n",
    "    \n",
    "    # Extract replies.\n",
    "    ext_tweets.at[index,'quotes'] = network['quote_count']\n",
    "    \n",
    "\n",
    "ext_tweets = ext_tweets.drop(['attachments','conversation_id','entities','lang','public_metrics','referenced_tweets','withheld','created_at'], axis=1, errors='ignore')\n",
    "\n",
    "ext_tweets = ext_tweets.rename(columns={\"geo\": \"tweet_loc\", \"text\": \"full_text\", \"id\":\"tweet_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Places: Merge and extract nested data, rename new rows.\n",
    "places = places.drop(['country_code','geo'], axis=1, errors='ignore')\n",
    "places = places.rename(columns={\"full_name\": \"tweet_loc_long\", \"name\": \"tweet_loc_short\", \"id\": \"geo_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets: Merge and extract nested data, rename new rows.\n",
    "for index, row in tweets.iterrows():\n",
    "    network = row['public_metrics']\n",
    "    \n",
    "    # Extract RTs.\n",
    "    tweets.at[index,'retweets'] = network['retweet_count']\n",
    "    \n",
    "    # Extract replies.\n",
    "    tweets.at[index,'replies'] = network['reply_count']\n",
    "    \n",
    "    # Extract replies.\n",
    "    tweets.at[index,'likes'] = network['like_count']\n",
    "    \n",
    "    # Extract replies.\n",
    "    tweets.at[index,'quotes'] = network['quote_count']\n",
    "\n",
    "tweets = tweets.drop(['lang','public_metrics','withheld'], axis=1, errors='ignore')\n",
    "tweets = tweets.rename(columns={'geo': 'geo_id', 'id': 'tweet_id'})\n",
    "\n",
    "# Remove irrelevant characters to match geo ID. \n",
    "tweets['geo_id'] = tweets['geo_id'].astype(str)\n",
    "tweets['geo_id'] = tweets['geo_id'].str[14:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tweets with places.\n",
    "tweets = pd.merge(tweets, places, how='left', on='geo_id')\n",
    "\n",
    "# Merge tweets with extended tweets.\n",
    "tweets = pd.merge(tweets, ext_tweets, how='left', on='tweet_id')\n",
    "\n",
    "# Drop irrelevant columns before large merge with users.\n",
    "tweets = tweets.rename(columns={'author_id_x': 'author_id'})\n",
    "tweets = tweets.drop(['author_id_y','in_reply_to_user_id_y','possibly_sensitive_y','reply_settings_y','source_y','retweets_y','replies_y','likes_y','quotes_y', ], axis=1, errors='ignore')\n",
    "\n",
    "# Drop duplicates to speed up tweets + users merge.\n",
    "tweets = tweets.drop_duplicates(subset=['tweet_id'])\n",
    "new_users = new_users.drop_duplicates(subset=['author_id'])\n",
    "\n",
    "tweets = pd.merge(tweets, new_users, how='left', on='author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns that would have generated after merge with users.\n",
    "tweets = tweets.drop(['author_id_y','in_reply_to_user_id_y','possibly_sensitive_y','reply_settings_y','source_y','retweets_y','replies_y','likes_y','quotes_y', ], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop in case any duplicates remain.\n",
    "tweets = tweets.drop_duplicates(subset=['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See total number of cleaned tweets.\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See a sample of tweets and rows to ensure it looks correct.\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Excel file.\n",
    "tweets.to_excel(\"combined-tweets.xlsx\")\n",
    "\n",
    "# Save as CSV file. Uncomment if this is the preferred option.\n",
    "# tweets.to_csv(\"combined-tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
